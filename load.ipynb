{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8636de-899b-4d1d-891e-8db46bc07b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feuilles disponibles dans le fichier Excel : ['Equipement', 'Feuil2']\n",
      "Chargement des données de la feuille 'Feuil2'.\n",
      "Exemple de données de la feuille 'Feuil2':\n",
      "        ARTI_CODE  ARTI_TCY_MOYEN ARTI_CALC_DELAI_MODE ARTI_CALC_DELAI  \\\n",
      "0            NaN             NaN                  NaN             NaN   \n",
      "1        4080052             NaN                  NaN               N   \n",
      "2      612085001             NaN                  NaN               N   \n",
      "3       GC6X4060             NaN                  NaN               N   \n",
      "4  KCP-612188000             NaN                  NaN               N   \n",
      "\n",
      "                           ARTI_DESIGNATION ARTI_SEVE_CODE ARTI_INDICE  \\\n",
      "0                                       NaN            NaN         NaN   \n",
      "1             HARNESS EASY FAST 8 CIL. OBII            NaN           1   \n",
      "2           CABL.CAVET.INTER CONNECT.SICMA2            NaN           3   \n",
      "3  GAINE EN PVC DIAM 6MM NOIRE 105° L4060MM            NaN         NaN   \n",
      "4     KCP-612188000 CABLAGE ELECTIRIQUE 56V            NaN         NaN   \n",
      "\n",
      "  ARTI_GESTION_SOURCE ARTI_GESTION_STOCK    ARTI_LIBELLE_COURT  ...  \\\n",
      "0                 NaN                NaN                   NaN  ...   \n",
      "1                   F                  O   HARNESS EASY FAST 8  ...   \n",
      "2                   F                  O  CABLAGE ELECTRIQUE 2  ...   \n",
      "3                   A                  O  GAINE EN PVC DIAM 6M  ...   \n",
      "4                   F                  O         KCP-612188000  ...   \n",
      "\n",
      "  ARTI_USER_NUM1_SAUV ARTI_BLOQUE_LANCEMENT ARTI_BLOQUE_LANC_COMM  \\\n",
      "0                 NaN                   NaN                   NaN   \n",
      "1                 NaN                     N                   NaN   \n",
      "2                 NaN                     N                   NaN   \n",
      "3                 NaN                     N                   NaN   \n",
      "4                 NaN                     N                   NaN   \n",
      "\n",
      "  ARTI_BLOQUE_LANC_PERS ARTI_BLOQUE_LANC_DT Unnamed: 208 Unnamed: 209  \\\n",
      "0                   NaN                 NaN          NaN          NaT   \n",
      "1                   NaN                 NaN          NaN          NaT   \n",
      "2                   NaN                 NaN          NaN          NaT   \n",
      "3                   NaN                 NaN          NaN          NaT   \n",
      "4                   NaN                 NaN          NaN          NaT   \n",
      "\n",
      "  Unnamed: 210 Unnamed: 211 Unnamed: 212  \n",
      "0          NaN          NaN          NaN  \n",
      "1          NaN          NaN          NaN  \n",
      "2          NaN          NaN          NaN  \n",
      "3          NaN          NaN          NaN  \n",
      "4          NaN          NaN          NaN  \n",
      "\n",
      "[5 rows x 213 columns]\n",
      "Types de colonnes de la feuille 'Feuil2':\n",
      " ARTI_CODE                 object\n",
      "ARTI_TCY_MOYEN           float64\n",
      "ARTI_CALC_DELAI_MODE      object\n",
      "ARTI_CALC_DELAI           object\n",
      "ARTI_DESIGNATION          object\n",
      "                          ...   \n",
      "ARTI_USER_NUM1_SAUV       object\n",
      "ARTI_BLOQUE_LANCEMENT     object\n",
      "ARTI_BLOQUE_LANC_COMM     object\n",
      "ARTI_BLOQUE_LANC_PERS     object\n",
      "ARTI_BLOQUE_LANC_DT       object\n",
      "Length: 208, dtype: object\n",
      "Données insérées avec succès dans la table Feuil2.\n"
     ]
    }
   ],
   "source": [
    "#__________________________________________chargement data.xls__________________________________________________________________\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Fonction pour charger un fichier Excel\n",
    "def load_excel(file_path):\n",
    "    # Lire le fichier Excel avec pandas\n",
    "    df = pd.read_excel(file_path, sheet_name=None)  # 'sheet_name=None' charge toutes les feuilles\n",
    "    return df\n",
    "\n",
    "# Fonction pour convertir les colonnes datetime en chaîne ou datetime si nécessaire\n",
    "def convert_datetime_columns(df):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Convertir les colonnes datetime en string (ou vous pouvez utiliser 'DATE' ou 'DATETIME' selon le besoin)\n",
    "            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')  # format datetime en string\n",
    "    return df\n",
    "\n",
    "# Fonction pour nettoyer les colonnes \"Unnamed\" et les renommer\n",
    "def clean_unnamed_columns(df):\n",
    "    # Supprimer les colonnes qui ont \"Unnamed\" dans leur nom\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "# Fonction pour charger les données dans SQL Server\n",
    "def load_to_sql_server(df, table_name, server, database, username, password):\n",
    "    connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Supprimer la table si elle existe\n",
    "        cursor.execute(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name};\")\n",
    "        conn.commit()\n",
    "        \n",
    "        # Créer la table avec des types adaptés\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "        {', '.join([f'[{col}] NVARCHAR(MAX)' if df[col].dtype == object else f'[{col}] FLOAT' for col in df.columns])}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Insérer les données dans la table\n",
    "        for index, row in df.iterrows():\n",
    "            placeholders = ', '.join(['?' for _ in row])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "            try:\n",
    "                values = [None if pd.isna(value) else value for value in row.values]\n",
    "                cursor.execute(insert_query, values)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'insertion de la ligne {index} : {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Données insérées avec succès dans la table {table_name}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur globale : {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Charger le fichier Excel\n",
    "file_path = r\"C:\\Users\\user\\Downloads\\Projet 2024\\Data.xlsx\"\n",
    "df = load_excel(file_path)\n",
    "\n",
    "# Afficher les noms des feuilles disponibles pour identifier la bonne feuille\n",
    "print(\"Feuilles disponibles dans le fichier Excel :\", list(df.keys()))\n",
    "\n",
    "# Vérifier si une feuille spécifique existe et charger les données\n",
    "sheet_name = 'Feuil2'  # Remplacer par le nom de la feuille que vous voulez utiliser\n",
    "if sheet_name in df:\n",
    "    df_sheet = df[sheet_name]\n",
    "    print(f\"Chargement des données de la feuille '{sheet_name}'.\")\n",
    "    print(f\"Exemple de données de la feuille '{sheet_name}':\\n\", df_sheet.head())  # Affiche un échantillon des données\n",
    "else:\n",
    "    print(f\"La feuille '{sheet_name}' n'existe pas dans le fichier Excel. Voici les feuilles disponibles : {list(df.keys())}\")\n",
    "    exit()\n",
    "\n",
    "# Nettoyer les colonnes \"Unnamed\"\n",
    "df_sheet = clean_unnamed_columns(df_sheet)\n",
    "\n",
    "# Convertir les colonnes datetime\n",
    "df_sheet = convert_datetime_columns(df_sheet)\n",
    "\n",
    "# Vérifier à nouveau les types de colonnes\n",
    "print(f\"Types de colonnes de la feuille '{sheet_name}':\\n\", df_sheet.dtypes)\n",
    "\n",
    "# Charger les données dans SQL Server\n",
    "load_to_sql_server(\n",
    "    df=df_sheet,\n",
    "    table_name=f'{sheet_name}',  # Nom de la table dans SQL Server\n",
    "    server='DESKTOP-U56S90R',\n",
    "    database='KomaxData',\n",
    "    username='sa',\n",
    "    password='yosr'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae98e03-1952-493e-bab8-f9fe9ef2cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\01\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\04\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\05\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\06\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\07\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\08\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\11\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\11\\Terminal.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\12\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\13\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\14\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\15\\Job.LDC\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\01\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\04\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\06\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\07\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\08\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\11\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\12\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\13\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\14\\Job.ldc\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\15\\Job.ldc\n",
      "Données insérées avec succès dans la table Job.\n",
      "Fichiers insérés dans la table :\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\01\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\04\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\05\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\06\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\07\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\08\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\11\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\11\\Terminal.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\12\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\13\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\14\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\433\\11\\15\\Job.LDC\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\01\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\04\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\06\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\07\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\08\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\11\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\12\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\13\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\14\\Job.ldc\n",
      "Fichier inséré : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9q0o_kd\\Data_komax\\530\\Nov\\15\\Job.ldc\n"
     ]
    }
   ],
   "source": [
    "#_________________________________chargement Job______________________________________________________\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import pyodbc\n",
    "\n",
    "# Fonction pour analyser et structurer les données pour IDC\n",
    "def parse_idc_data(text):\n",
    "    data = []\n",
    "    lines = text.strip().splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        columns = line.split('\\t')\n",
    "        if len(columns) >= 6:\n",
    "            # Vérifier et séparer les valeurs contenant des virgules dans des colonnes séparées\n",
    "            identifier_values = columns[4].split(',') if ',' in columns[4] else [columns[4]]\n",
    "            status_values = columns[5].split(',') if ',' in columns[5] else [columns[5]]\n",
    "            \n",
    "            # Ajouter des colonnes supplémentaires si nécessaire\n",
    "            row = {\n",
    "                'Date': columns[0],\n",
    "                'Time': columns[1],\n",
    "                'Section': columns[2],\n",
    "                'Action': columns[3],\n",
    "                'Identifier1': identifier_values[0],  # Première valeur de l'identifiant\n",
    "                'Identifier2': identifier_values[1] if len(identifier_values) > 1 else None,  # Seconde valeur de l'identifiant (si elle existe)\n",
    "                'Status1': status_values[0],  # Première valeur du statut\n",
    "                'Status2': status_values[1] if len(status_values) > 1 else None,  # Seconde valeur du statut (si elle existe)\n",
    "            }\n",
    "            data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Fonction pour analyser et structurer les données pour LDC\n",
    "def parse_ldc_data(text):\n",
    "    data = []\n",
    "    lines = text.strip().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        columns = line.split('\\t')  # Séparation par tabulation\n",
    "        if len(columns) >= 6:\n",
    "            # Vérifier et séparer les valeurs contenant des virgules dans des colonnes séparées\n",
    "            identifier_values = columns[4].split(',') if ',' in columns[4] else [columns[4]]\n",
    "            status_values = columns[5].split(',') if ',' in columns[5] else [columns[5]]\n",
    "            \n",
    "            # Ajouter des colonnes supplémentaires si nécessaire\n",
    "            row = {\n",
    "                'Date': columns[0],\n",
    "                'Time': columns[1],\n",
    "                'Section': columns[2],\n",
    "                'Action': columns[3],\n",
    "                'Identifier1': identifier_values[0],  # Première valeur de l'identifiant\n",
    "                'Identifier2': identifier_values[1] if len(identifier_values) > 1 else None,  # Seconde valeur de l'identifiant (si elle existe)\n",
    "                'Status1': status_values[0],  # Première valeur du statut\n",
    "                'Status2': status_values[1] if len(status_values) > 1 else None,  # Seconde valeur du statut (si elle existe)\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Fonction pour traiter les fichiers ZIP et détecter le type de fichier (IDC ou LDC)\n",
    "def process_zip(zip_path):\n",
    "    all_data = []\n",
    "    inserted_files = []  # Liste pour stocker les fichiers insérés\n",
    "    columns_set = set()  # Set pour collecter toutes les colonnes rencontrées\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(temp_dir)\n",
    "        \n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file.lower().endswith(('.idc', '.ldc')):  # Vérification insensible à la casse\n",
    "                    print(f\"Fichier trouvé : {file_path}\")  # Affiche le chemin du fichier trouvé\n",
    "                    try:\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            text = f.read()\n",
    "\n",
    "                        if 'idc' in file.lower():  # Si le fichier est IDC\n",
    "                            df = parse_idc_data(text)\n",
    "                        else:  # Si le fichier est LDC\n",
    "                            df = parse_ldc_data(text)\n",
    "\n",
    "                        # Ajouter la colonne FilePath pour suivre le fichier\n",
    "                        df['FilePath'] = file_path\n",
    "                        all_data.append(df)\n",
    "                        \n",
    "                        # Ajouter à la liste des fichiers insérés\n",
    "                        inserted_files.append(file_path)\n",
    "\n",
    "                        # Mettre à jour le set des colonnes rencontrées\n",
    "                        columns_set.update(df.columns)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors du traitement du fichier {file_path}: {e}\")\n",
    "    \n",
    "    # Retourner les données et les fichiers insérés\n",
    "    if all_data:\n",
    "        # Aligner les colonnes\n",
    "        for i, df in enumerate(all_data):\n",
    "            missing_columns = columns_set - set(df.columns)  # Colonnes manquantes\n",
    "            for col in missing_columns:\n",
    "                df[col] = None  # Ajouter des colonnes manquantes avec des valeurs None\n",
    "            all_data[i] = df[list(columns_set)]  # Réorganiser les colonnes pour être les mêmes partout\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df, inserted_files\n",
    "    else:\n",
    "        return pd.DataFrame(), inserted_files\n",
    "\n",
    "# Fonction pour nettoyer et préparer le DataFrame avant insertion dans SQL Server\n",
    "def clean_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(None)\n",
    "    return df\n",
    "\n",
    "# Fonction pour charger les données dans SQL Server\n",
    "def load_to_sql_server(df, table_name, server, database, username, password, inserted_files):\n",
    "    connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Supprimer la table si elle existe\n",
    "        drop_table_query = f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name};\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        conn.commit()\n",
    "\n",
    "        # Créer la table avec des colonnes dynamiques\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "        {', '.join([f'[{col}] NVARCHAR(MAX)' if df[col].dtype == object else f'[{col}] FLOAT' for col in df.columns])}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "\n",
    "        # Insérer les données ligne par ligne\n",
    "        for index, row in df.iterrows():\n",
    "            placeholders = ', '.join(['?' for _ in row])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "            cursor.execute(insert_query, *row.values.tolist())\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Données insérées avec succès dans la table {table_name}.\")\n",
    "        \n",
    "        # Afficher les fichiers insérés\n",
    "        print(\"Fichiers insérés dans la table :\")\n",
    "        for file in inserted_files:\n",
    "            print(f\"Fichier inséré : {file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données : {e}\")\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# Exemple de chemin vers le fichier ZIP\n",
    "zip_path = r'C:/Users/user/Downloads/Projet 2024.zip'\n",
    "\n",
    "# Traiter l'archive ZIP\n",
    "df_combined, inserted_files = process_zip(zip_path)\n",
    "\n",
    "# Charger les données dans SQL Server\n",
    "if not df_combined.empty:\n",
    "    # Nettoyer le DataFrame\n",
    "    df_combined = clean_dataframe(df_combined)\n",
    "    \n",
    "    load_to_sql_server(\n",
    "        df=df_combined,\n",
    "        table_name='Job',  # Nouveau nom de la table\n",
    "        server='DESKTOP-U56S90R',\n",
    "        database='KomaxData',\n",
    "        username='sa',\n",
    "        password='yosr',\n",
    "        inserted_files=inserted_files  # Passer les fichiers insérés à la fonction\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea0d7eb-fefb-466e-867c-988a8f06ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feuilles disponibles dans le fichier Excel : ['Equipement', 'Feuil2']\n",
      "Chargement des données de la feuille 'Equipement'.\n",
      "Données insérées avec succès dans la table Equipement_Table.\n"
     ]
    }
   ],
   "source": [
    "#___________________________________________________chargement Equipement_____________________________________________________\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Fonction pour charger un fichier Excel\n",
    "def load_excel(file_path):\n",
    "    # Lire le fichier Excel avec pandas\n",
    "    df = pd.read_excel(file_path, sheet_name=None)  # 'sheet_name=None' charge toutes les feuilles\n",
    "    return df\n",
    "\n",
    "# Fonction pour convertir les colonnes datetime en chaîne ou datetime si nécessaire\n",
    "def convert_datetime_columns(df):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            # Convertir les colonnes datetime en string (ou vous pouvez utiliser 'DATE' ou 'DATETIME' selon le besoin)\n",
    "            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')  # format datetime en string\n",
    "    return df\n",
    "\n",
    "# Fonction pour charger les données dans SQL Server\n",
    "def load_to_sql_server(df, table_name, server, database, username, password):\n",
    "    connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Supprimer la table si elle existe\n",
    "        cursor.execute(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name};\")\n",
    "        conn.commit()\n",
    "        \n",
    "        # Créer la table avec des types adaptés\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "        {', '.join([f'[{col}] NVARCHAR(MAX)' if df[col].dtype == object else f'[{col}] FLOAT' for col in df.columns])}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Insérer les données dans la table\n",
    "        for index, row in df.iterrows():\n",
    "            placeholders = ', '.join(['?' for _ in row])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "            try:\n",
    "                values = [None if pd.isna(value) else value for value in row.values]\n",
    "                cursor.execute(insert_query, values)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'insertion de la ligne {index} : {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Données insérées avec succès dans la table {table_name}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur globale : {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Charger le fichier Excel\n",
    "file_path = r\"C:\\Users\\user\\Downloads\\Projet 2024\\Data.xlsx\"\n",
    "df = load_excel(file_path)\n",
    "\n",
    "# Afficher les noms des feuilles disponibles pour identifier la bonne feuille\n",
    "print(\"Feuilles disponibles dans le fichier Excel :\", list(df.keys()))\n",
    "\n",
    "# Vérifier si une feuille spécifique existe\n",
    "sheet_name = 'Equipement'  # Remplacez par le nom de la feuille que vous voulez utiliser\n",
    "if sheet_name in df:\n",
    "    df_sheet = df[sheet_name]\n",
    "    print(f\"Chargement des données de la feuille '{sheet_name}'.\")\n",
    "else:\n",
    "    print(f\"La feuille '{sheet_name}' n'existe pas dans le fichier Excel. Voici les feuilles disponibles : {list(df.keys())}\")\n",
    "    exit()\n",
    "\n",
    "# Convertir les colonnes datetime\n",
    "df_sheet = convert_datetime_columns(df_sheet)\n",
    "\n",
    "# Charger les données dans SQL Server\n",
    "load_to_sql_server(\n",
    "    df=df_sheet,\n",
    "    table_name='Equipement_Table',  # Nom de la table dans SQL Server\n",
    "    server='DESKTOP-U56S90R',\n",
    "    database='KomaxData',\n",
    "    username='sa',\n",
    "    password='yosr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e33242-a870-433e-8b33-12675b407c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données insérées avec succès dans la table BESOIN_OF_H_DATA_TABLE.\n"
     ]
    }
   ],
   "source": [
    "#_____________________________________________table BESOIN_OF_H_DATA_TABLE_____________________________________________________________________\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import re\n",
    "\n",
    "# Fonction pour détecter et corriger les erreurs dans le fichier CSV\n",
    "def load_and_clean_csv(file_path):\n",
    "    # Charger le fichier en tant que texte pour détecter les lignes problématiques\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Remplacer les séquences de virgules multiples (',,,') par une seule virgule\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Remplacer les virgules consécutives par une seule virgule\n",
    "        line = re.sub(r',+', ',', line)\n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    # Sauvegarder le fichier nettoyé temporairement\n",
    "    cleaned_file_path = file_path.replace('.csv', '_cleaned.csv')\n",
    "    with open(cleaned_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "    \n",
    "    # Charger le fichier nettoyé dans un DataFrame\n",
    "    df = pd.read_csv(cleaned_file_path, low_memory=False)\n",
    "    return df\n",
    "\n",
    "# Fonction pour nettoyer le DataFrame\n",
    "def clean_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            # Remplacer les NaN dans les colonnes texte par une chaîne vide\n",
    "            df[col] = df[col].fillna(\"\").astype(str)\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Remplacer les NaN et valeurs infinies dans les colonnes numériques\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Convertit les erreurs en NaN\n",
    "            df[col] = df[col].fillna(np.nan).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "# Fonction pour charger les données dans SQL Server\n",
    "def load_to_sql_server(df, table_name, server, database, username, password):\n",
    "    connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Supprimer la table si elle existe\n",
    "        drop_table_query = f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name};\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Création de la table\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "        {', '.join([f'[{col}] FLOAT' if pd.api.types.is_numeric_dtype(df[col]) else f'[{col}] NVARCHAR(MAX)' for col in df.columns])}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Insérer les données ligne par ligne\n",
    "        for index, row in df.iterrows():\n",
    "            placeholders = ', '.join(['?' for _ in row])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "            try:\n",
    "                cursor.execute(insert_query, *[None if pd.isna(value) else value for value in row.values])\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'insertion de la ligne {index}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Données insérées avec succès dans la table {table_name}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur globale : {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Charger le fichier CSV et corriger les erreurs\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\developer\\sqldeveloper\\sqldeveloper\\bin\\BESOIN_OF_H_DATA_TABLE.csv\"\n",
    "df = load_and_clean_csv(csv_file_path)\n",
    "\n",
    "# Nettoyer le DataFrame\n",
    "df_cleaned = clean_dataframe(df)\n",
    "\n",
    "# Charger les données dans SQL Server\n",
    "load_to_sql_server(\n",
    "    df=df_cleaned,\n",
    "    table_name='BESOIN_OF_H_DATA_TABLEeeeeee',  # Nom de la table\n",
    "    server='DESKTOP-U56S90R',\n",
    "    database='Data_Komax',\n",
    "    username='sa',\n",
    "    password='yosr'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db27f0ff-9b31-46cc-91d5-9feccbf7e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#___________________________________________________BESOIN_OF_GROF_H_DATA_TABLE_________________________________________________________________________\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "\n",
    "# Fonction pour nettoyer le DataFrame\n",
    "def clean_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            # Remplacer les NaN dans les colonnes texte par une chaîne vide\n",
    "            df[col] = df[col].fillna(\"\").astype(str)\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Remplacer les NaN et valeurs infinies dans les colonnes numériques\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Convertit les erreurs en NaN\n",
    "            df[col] = df[col].fillna(np.nan).replace([np.inf, -np.inf], np.nan)\n",
    "    return df\n",
    "\n",
    "# Fonction pour charger les données dans SQL Server\n",
    "def load_to_sql_server(df, table_name, server, database, username, password):\n",
    "    connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Supprimer la table si elle existe\n",
    "        drop_table_query = f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name};\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Création de la table\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "        {', '.join([f'[{col}] FLOAT' if pd.api.types.is_numeric_dtype(df[col]) else f'[{col}] NVARCHAR(MAX)' for col in df.columns])}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        \n",
    "        # Insérer les données ligne par ligne\n",
    "        for index, row in df.iterrows():\n",
    "            placeholders = ', '.join(['?' for _ in row])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "            try:\n",
    "                cursor.execute(insert_query, *[None if pd.isna(value) else value for value in row.values])\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'insertion de la ligne {index}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur globale : {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Charger le fichier TSV (en utilisant un délimiteur tabulation)\n",
    "tsv_file_path = r\"C:\\Users\\user\\Desktop\\developer\\sqldeveloper\\sqldeveloper\\bin\\BESOIN_OF_GROF_H_DATA_TABLE.tsv\"\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t', low_memory=False)  # Spécifier le séparateur tabulation\n",
    "\n",
    "# Nettoyer le DataFrame\n",
    "df_cleaned = clean_dataframe(df)\n",
    "\n",
    "# Charger les données dans SQL Server\n",
    "load_to_sql_server(\n",
    "    df=df_cleaned,\n",
    "    table_name='BESOIN_OF_GROF_H_DATA_TABLE',\n",
    "    server='DESKTOP-U56S90R',\n",
    "    database='KomaxData',\n",
    "    username='sa',\n",
    "    password='yosr'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff5e97b-9e1e-4006-aed0-4fd4a77bd1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\01\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\01\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\04\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\04\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\05\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\05\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\06\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\06\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\07\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\07\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\08\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\08\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\11\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\12\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\12\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\13\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\13\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\14\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\14\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\15\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\433\\11\\15\\Producti.SDC\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\01\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\01\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\04\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\04\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\06\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\06\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\07\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\07\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\08\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\08\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\11\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\11\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\12\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\12\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\13\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\13\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\14\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\14\\Producti.sdc\n",
      "Exploration : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\15\n",
      "Fichier trouvé : C:\\Users\\user\\AppData\\Local\\Temp\\tmpb9ow_pp3\\Data_komax\\530\\Nov\\15\\Producti.sdc\n",
      "\n",
      "--- Données combinées ---\n",
      "            Section DateTimeStamp_1 DateTimeStamp_2 ArticleKey     Job_1  \\\n",
      "0     SampleStarted      01/11/2024        06:31:28   57628_-2  57628_-2   \n",
      "1  SampleTerminated      01/11/2024        06:31:40   57628_-2  57628_-2   \n",
      "2      LearnStarted      01/11/2024        06:32:19   57628_-2  57628_-2   \n",
      "3      LearnAborted      01/11/2024        06:32:30   57628_-2  57628_-2   \n",
      "4     SampleStarted      01/11/2024        06:36:26   57628_-2  57628_-2   \n",
      "\n",
      "  Job_2  UserName SampleRequestedPieces     Wire_1   Wire_2  ...  \\\n",
      "0     1  MED AMIN                     1  CU04CR001  2904305  ...   \n",
      "1     1  MED AMIN                     1  CU04CR001  2904625  ...   \n",
      "2     1  MED AMIN                   NaN  CU04CR001  2904625  ...   \n",
      "3     1  MED AMIN                   NaN  CU04CR001  2904625  ...   \n",
      "4     1  MED AMIN                     1  CU04CR001  2904945  ...   \n",
      "\n",
      "  TerminalEnd2_5 TerminalEnd2_6 TerminalEnd2_7 TerminalEnd2_8 TerminalEnd2_9  \\\n",
      "0            NaN            NaN            NaN            NaN            NaN   \n",
      "1            NaN            NaN            NaN            NaN            NaN   \n",
      "2            NaN            NaN            NaN            NaN            NaN   \n",
      "3            NaN            NaN            NaN            NaN            NaN   \n",
      "4            NaN            NaN            NaN            NaN            NaN   \n",
      "\n",
      "  TerminalEnd2_10 TerminalEnd2_11 TerminalEnd2_12 TerminalEnd2_13  \\\n",
      "0             NaN             NaN             NaN             NaN   \n",
      "1             NaN             NaN             NaN             NaN   \n",
      "2             NaN             NaN             NaN             NaN   \n",
      "3             NaN             NaN             NaN             NaN   \n",
      "4             NaN             NaN             NaN             NaN   \n",
      "\n",
      "  TerminalEnd2_14  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "\n",
      "[5 rows x 88 columns]\n",
      "Table Producti supprimée si elle existait.\n",
      "Table Producti créée avec succès.\n",
      "Données insérées avec succès dans la table Producti.\n"
     ]
    }
   ],
   "source": [
    "#____________________________________________________chargement KomaxData_______________________________________________________________________________\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import tempfile\n",
    "import pyodbc  # Pour la connexion à SQL Server\n",
    "\n",
    "# Fonction pour analyser et structurer les données\n",
    "def parse_data(text):\n",
    "    data = []\n",
    "    sections = re.split(r'\\[([^\\]]+)\\]', text.strip())  # Diviser par sections\n",
    "    last_row = None  # Pour conserver la ligne précédente\n",
    "\n",
    "    for i in range(1, len(sections), 2):\n",
    "        section_name = sections[i].strip()  # Le nom de la section\n",
    "        content = sections[i + 1].strip().splitlines()  # Contenu de la section\n",
    "\n",
    "        if section_name.lower() == \"counter\" and last_row is not None:\n",
    "            # Ajouter les données de la section \"Counter\" à la dernière ligne\n",
    "            for line in content:\n",
    "                if '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    values = [v.strip() for v in value.split(',')]\n",
    "\n",
    "                    # Ajouter chaque valeur comme colonne unique\n",
    "                    if len(values) > 1:\n",
    "                        for idx, v in enumerate(values, start=1):\n",
    "                            col_name = f\"{key.strip()}_{idx}\"  # Nom de colonne unique\n",
    "                            last_row[col_name] = v\n",
    "                    else:\n",
    "                        last_row[key.strip()] = value.strip()\n",
    "        else:\n",
    "            # Créer une nouvelle ligne pour les autres sections\n",
    "            row = {\"Section\": section_name}\n",
    "            for line in content:\n",
    "                if '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    values = [v.strip() for v in value.split(',')]\n",
    "\n",
    "                    # Ajouter chaque valeur comme colonne unique\n",
    "                    if len(values) > 1:\n",
    "                        for idx, v in enumerate(values, start=1):\n",
    "                            col_name = f\"{key.strip()}_{idx}\"\n",
    "                            row[col_name] = v\n",
    "                    else:\n",
    "                        row[key.strip()] = value.strip()\n",
    "\n",
    "            data.append(row)\n",
    "            last_row = row  # Mettre à jour la dernière ligne\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Fonction pour nettoyer le DataFrame\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"\n",
    "    Nettoyer et préparer le DataFrame avant insertion dans SQL Server.\n",
    "    - Convertir les colonnes en types compatibles.\n",
    "    - Remplacer uniquement les valeurs NaN par des valeurs appropriées.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            # Remplacer les NaN dans les colonnes texte par une chaîne vide\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Remplacer les NaN dans les colonnes numériques par None (géré par SQL comme NULL)\n",
    "            df[col] = df[col].fillna(None)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fonction pour traiter les fichiers dans un fichier ZIP\n",
    "def process_zip(zip_path):\n",
    "    all_data = []\n",
    "\n",
    "    # Créer un répertoire temporaire pour extraire les fichiers ZIP\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(temp_dir)\n",
    "\n",
    "        # Explorer les fichiers extraits\n",
    "        for root, dirs, files in os.walk(temp_dir):\n",
    "            print(f\"Exploration : {root}\")\n",
    "            for file in files:\n",
    "                if file.endswith(('.sdc', '.SDC')):  # Traiter les fichiers avec .sdc ou .SDC\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    print(f\"Fichier trouvé : {file_path}\")\n",
    "\n",
    "                    # Lire le contenu du fichier\n",
    "                    try:\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            text = f.read()\n",
    "\n",
    "                        # Appliquer la fonction de parsing\n",
    "                        df = parse_data(text)\n",
    "                        df['FilePath'] = file_path  # Ajouter une colonne avec le chemin du fichier\n",
    "                        all_data.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors du traitement du fichier {file_path}: {e}\")\n",
    "\n",
    "    # Combiner toutes les données en un seul DataFrame\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(\"\\n--- Données combinées ---\")\n",
    "        print(combined_df.head())  # Afficher les premières lignes du DataFrame\n",
    "\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"Aucune donnée trouvée pour les fichiers .sdc\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fonction pour charger les données dans SQL Server\n",
    "def load_to_sql_server(df, table_name, server, database, username, password):\n",
    "    connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    try:\n",
    "        # Connexion à la base de données\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Supprimer la table si elle existe\n",
    "        drop_table_query = f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name};\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} supprimée si elle existait.\")\n",
    "\n",
    "        # Création de la table\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "        {', '.join([f'[{col}] FLOAT' if pd.api.types.is_numeric_dtype(df[col]) else f'[{col}] NVARCHAR(MAX)' for col in df.columns])}\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} créée avec succès.\")\n",
    "\n",
    "        # Insérer les données ligne par ligne\n",
    "        for index, row in df.iterrows():\n",
    "            placeholders = ', '.join(['?' for _ in row])\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "            cursor.execute(insert_query, *row.values.tolist())\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Données insérées avec succès dans la table {table_name}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données : {e}\")\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Chemin du fichier ZIP contenant d'autres fichiers .sdc\n",
    "zip_path = r'C:/Users/user/Downloads/Projet 2024.zip'\n",
    "\n",
    "# Appeler la fonction pour traiter l'archive ZIP\n",
    "df_combined = process_zip(zip_path)\n",
    "\n",
    "# Charger les données dans SQL Server\n",
    "if not df_combined.empty:\n",
    "    # Nettoyer le DataFrame\n",
    "    df_combined = clean_dataframe(df_combined)\n",
    "\n",
    "    load_to_sql_server(\n",
    "        df=df_combined,\n",
    "        table_name='Producti',\n",
    "        server='DESKTOP-U56S90R',\n",
    "        database='KomaxData',\n",
    "        username='sa',\n",
    "        password='yosr'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21acf2f-2042-43db-9ae7-9941e8ee8943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données ont été transférées avec succès.\n"
     ]
    }
   ],
   "source": [
    "#_________________________________insertion de New000KomaxData pour charger vers chargeods________________________________\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configuration de la connexion SQL Server\n",
    "server = 'DESKTOP-U56S90R'\n",
    "database = 'Data_Komax'\n",
    "username = 'sa'\n",
    "password = 'yosr'\n",
    "\n",
    "# Connexion via SQLAlchemy\n",
    "engine = create_engine(f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "\n",
    "# Étape 1: Lire les colonnes de la table source\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "      [Section],\n",
    "      [DateTimeStamp_1],\n",
    "      [DateTimeStamp_2],\n",
    "      [ArticleKey],\n",
    "      [Job_1],\n",
    "      [Job_2],\n",
    "      [UserName],\n",
    "      [SampleRequestedPieces],\n",
    "      [SampleStartedCounterDateTimeStamp_1],\n",
    "      [SampleStartedCounterDateTimeStamp_2],\n",
    "      [SampleStartedCounterWire_1],\n",
    "      [SampleStartedCounterWire_2],\n",
    "      [SampleStartedCounterWire_3],\n",
    "      [SampleStartedCounterTerminal_1],\n",
    "      [SampleStartedCounterTerminal_2],\n",
    "      [SampleStartedCounterTerminal_3],\n",
    "      [SampleTerminatedCounterDateTimeStamp_1],\n",
    "      [SampleTerminatedCounterDateTimeStamp_2],\n",
    "      [SampleTerminatedCounterWire_1],\n",
    "      [SampleTerminatedCounterWire_2],\n",
    "      [SampleTerminatedCounterWire_3],\n",
    "      [SampleTerminatedCounterTerminal_1],\n",
    "      [SampleTerminatedCounterTerminal_2],\n",
    "      [SampleTerminatedCounterTerminal_3],\n",
    "      [LearnStartedCounterDateTimeStamp_1],\n",
    "      [LearnStartedCounterDateTimeStamp_2],\n",
    "      [LearnStartedCounterWire_1],\n",
    "      [LearnStartedCounterWire_2],\n",
    "      [LearnStartedCounterWire_3],\n",
    "      [LearnStartedCounterTerminal_1],\n",
    "      [LearnStartedCounterTerminal_2],\n",
    "      [LearnStartedCounterTerminal_3],\n",
    "      [LearnAbortedCounterDateTimeStamp_1],\n",
    "      [LearnAbortedCounterDateTimeStamp_2],\n",
    "      [LearnAbortedCounterWire_1],\n",
    "      [LearnAbortedCounterWire_2],\n",
    "      [LearnAbortedCounterWire_3],\n",
    "      [LearnAbortedCounterTerminal_1],\n",
    "      [LearnAbortedCounterTerminal_2],\n",
    "      [LearnAbortedCounterTerminal_3],\n",
    "      [LearnTerminatedCounterDateTimeStamp_1],\n",
    "      [LearnTerminatedCounterDateTimeStamp_2],\n",
    "      [LearnTerminatedCounterWire_1],\n",
    "      [LearnTerminatedCounterWire_2],\n",
    "      [LearnTerminatedCounterWire_3],\n",
    "      [LearnTerminatedCounterTerminal_1],\n",
    "      [LearnTerminatedCounterTerminal_2],\n",
    "      [LearnTerminatedCounterTerminal_3],\n",
    "      [ProductionRequestedPieces],\n",
    "      [ProductionStartedCounterDateTimeStamp_1],\n",
    "      [ProductionStartedCounterDateTimeStamp_2],\n",
    "      [ProductionStartedCounterWire_1],\n",
    "      [ProductionStartedCounterWire_2],\n",
    "      [ProductionStartedCounterWire_3],\n",
    "      [ProductionStartedCounterTerminal_1],\n",
    "      [ProductionStartedCounterTerminal_2],\n",
    "      [ProductionStartedCounterTerminal_3],\n",
    "      [TotalGoodPieces],\n",
    "      [ProductionInterruptedCounterDateTimeStamp_1],\n",
    "      [ProductionInterruptedCounterDateTimeStamp_2],\n",
    "      [ProductionInterruptedCounterWire_1],\n",
    "      [ProductionInterruptedCounterWire_2],\n",
    "      [ProductionInterruptedCounterWire_3],\n",
    "      [ProductionInterruptedCounterTerminal_1],\n",
    "      [ProductionInterruptedCounterTerminal_2],\n",
    "      [ProductionInterruptedCounterTerminal_3],\n",
    "      [ProductionRestartedCounterDateTimeStamp_1],\n",
    "      [ProductionRestartedCounterDateTimeStamp_2],\n",
    "      [ProductionRestartedCounterWire_1],\n",
    "      [ProductionRestartedCounterWire_2],\n",
    "      [ProductionRestartedCounterWire_3],\n",
    "      [ProductionRestartedCounterTerminal_1],\n",
    "      [ProductionRestartedCounterTerminal_2],\n",
    "      [ProductionRestartedCounterTerminal_3],\n",
    "      [UserRequestedPieces],\n",
    "      [ProductionTerminatedCounterDateTimeStamp_1],\n",
    "      [ProductionTerminatedCounterDateTimeStamp_2],\n",
    "      [ProductionTerminatedCounterWire_1],\n",
    "      [ProductionTerminatedCounterWire_2],\n",
    "      [ProductionTerminatedCounterWire_3],\n",
    "      [ProductionTerminatedCounterTerminal_1],\n",
    "      [ProductionTerminatedCounterTerminal_2],\n",
    "      [ProductionTerminatedCounterTerminal_3],\n",
    "      [SampleAbortedCounterDateTimeStamp_1],\n",
    "      [SampleAbortedCounterDateTimeStamp_2],\n",
    "      [SampleAbortedCounterWire_1],\n",
    "      [SampleAbortedCounterWire_2],\n",
    "      [SampleAbortedCounterWire_3],\n",
    "      [SampleAbortedCounterTerminal_1],\n",
    "      [SampleAbortedCounterTerminal_2],\n",
    "      [SampleAbortedCounterTerminal_3],\n",
    "      [SampleStartedCounterSeal_1],\n",
    "      [SampleStartedCounterSeal_2],\n",
    "      [SampleStartedCounterSeal_3],\n",
    "      [SampleTerminatedCounterSeal_1],\n",
    "      [SampleTerminatedCounterSeal_2],\n",
    "      [SampleTerminatedCounterSeal_3],\n",
    "      [QualityParametersCounterDateTimeStamp_1],\n",
    "      [QualityParametersCounterDateTimeStamp_2],\n",
    "      [QualityParametersCounterWire_1],\n",
    "      [QualityParametersCounterWire_2],\n",
    "      [QualityParametersCounterWire_3],\n",
    "      [QualityParametersCounterTerminal_1],\n",
    "      [QualityParametersCounterTerminal_2],\n",
    "      [QualityParametersCounterTerminal_3],\n",
    "      [Event],\n",
    "      [LearnStartedCounterSeal_1],\n",
    "      [LearnStartedCounterSeal_2],\n",
    "      [LearnStartedCounterSeal_3],\n",
    "      [LearnTerminatedCounterSeal_1],\n",
    "      [LearnTerminatedCounterSeal_2],\n",
    "      [LearnTerminatedCounterSeal_3],\n",
    "      [ProductionPieces_1],\n",
    "      [ProductionPieces_2],\n",
    "      [ProductionStartedCounterSeal_1],\n",
    "      [ProductionStartedCounterSeal_2],\n",
    "      [ProductionStartedCounterSeal_3],\n",
    "      [ProductionInterruptedCounterSeal_1],\n",
    "      [ProductionInterruptedCounterSeal_2],\n",
    "      [ProductionInterruptedCounterSeal_3],\n",
    "      [Seal],\n",
    "      [ProductionRestartedCounterSeal_1],\n",
    "      [ProductionRestartedCounterSeal_2],\n",
    "      [ProductionRestartedCounterSeal_3]\n",
    "FROM [dbo].[KomaxData]\n",
    "\"\"\"\n",
    "\n",
    "# Lire les données\n",
    "data = pd.read_sql_query(query, engine)\n",
    "\n",
    "# Étape 2: Insérer les données dans une nouvelle table\n",
    "#table_cible = 'New000KomaxData'\n",
    "table_cible = 'Producti'\n",
    "\n",
    "# Insérer directement en utilisant SQLAlchemy\n",
    "data.to_sql(name=table_cible, con=engine, if_exists='append', index=False, chunksize=1000)\n",
    "\n",
    "print(\"Les données ont été transférées avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29078b3-ebff-4fc9-a3da-cb852d52c7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
